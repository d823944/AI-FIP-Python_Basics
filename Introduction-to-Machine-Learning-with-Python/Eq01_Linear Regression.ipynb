{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definition for symbols \n",
    "\n",
    "$X_{j}^{(i)}$: the j-th feature of the i-th data point. ($i = 1 \\cdots m$, $j = 1 \\cdots n$)\n",
    "\n",
    "$m$: number of data points\n",
    "\n",
    "$n$: number of features\n",
    "\n",
    "$y^{(i)}$: true traget value (or target label) of the i-th data point.\n",
    "\n",
    "$\\theta_{j}$: the j-th hypothesis parameter of the model.\n",
    "\n",
    "$\\hat{y^{(i)}}$: predicted value (or label) of the i-th data point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Regression (one feature)\n",
    "$$ \\hat{y^{(i)}} = \\theta_0 + X_1^{(i)} \\cdot \\theta_1  $$\n",
    "\n",
    "$$ = 1 \\cdot \\theta_0 + X_1^{(i)} \\cdot \\theta_1$$\n",
    "\n",
    "$$ = X_0^{(i)} \\cdot \\theta_0 + X_1^{(i)} \\cdot \\theta_1$$\n",
    "\n",
    "$(X_0 = 1, i = 1 \\cdots m)$\n",
    "\n",
    "$\\theta_1$: slope\n",
    "\n",
    "$\\theta_0$: interception\n",
    "\n",
    "$$\\hat{y} = X \\cdot \\theta$$\n",
    "\n",
    "$\\hat{y} = \\begin{bmatrix}\n",
    "    \\hat{y^{(1)}} \\\\\n",
    "    \\hat{y^{(2)}} \\\\\n",
    "    \\vdots \\\\\n",
    "    \\hat{y^{(m)}}\n",
    "    \\end{bmatrix}$, \n",
    "$X = \\begin{bmatrix}\n",
    "    1 & X_1^{(1)}\\\\\n",
    "    1 & X_1^{(2)}\\\\\n",
    "    \\vdots\\\\\n",
    "    1 & X_1^{(m)}\n",
    "    \\end{bmatrix}$, \n",
    "$\\theta = \\begin{bmatrix}\n",
    "    \\theta_0 \\\\\n",
    "    \\theta_1 \\\\\n",
    "    \\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cost function $J(\\theta)$ ($n$ features)\n",
    "MSE (mean square error)\n",
    "$$ J(\\theta) = \\frac{1}{2m} \\sum_{i=1}^m (\\hat{y^{(i)}} - y^{(i)})^2$$\n",
    "\n",
    "$$ = \\frac{1}{2m} \\sum_{i=1}^m (X^{(i)} \\cdot \\theta - y^{(i)})^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Descent Search: update $\\theta$ to minimize $J(\\theta)$\n",
    "$$ \\theta_j := \\theta_j - \\alpha \\frac{\\partial}{\\partial \\theta_j} J(\\theta)$$\n",
    "\n",
    "$$ = \\theta_j - \\frac{\\alpha}{m} \\sum_{i=1}^m (X^{(i)} \\cdot \\theta - y^{(i)}) X_j^{(i)}$$\n",
    "\n",
    "$\\alpha$: learning rate\n",
    "\n",
    "$j = 0 \\cdots n$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normal equation\n",
    "The MSE $J(\\theta)$ is a convex function and has a global minimum.\n",
    "\n",
    "When reaching the global minimum of $J(\\theta)$, it means $\\theta$ converges to an optimal value and means\n",
    "$$\\frac{\\partial}{\\partial \\theta_j} J(\\theta) = 0$$\n",
    "\n",
    "$$X^T (X \\cdot \\theta - y) = 0$$\n",
    "\n",
    "The closed-form solution to linear regression is\n",
    "$$\\theta = (X^T X)^{-1} X^T y$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ex1data1.txt is a dataset with the following information\n",
    "Dataset: 97 rows (data points), 2 columns (1 feature & 1 target value)\n",
    "\n",
    "The feature for training model and prediction: Population of City in 10,000s\n",
    "\n",
    "Target value: Profit (of food truck) in $10,000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotData(X, y):\n",
    "    \"\"\"Function to plot classification data\n",
    "    Parameters\n",
    "    ---------\n",
    "        X: Data matrix. X.shape = (m, n)\n",
    "        y: Target vector. y.shape = (m, 1)\n",
    "    \"\"\"\n",
    "    plt.scatter(X, y, c='r', marker='x', s=20)\n",
    "    plt.xlabel('Population of City in 10,000s')\n",
    "    plt.ylabel('Profit in $10,000')\n",
    "\n",
    "data = np.loadtxt('ex1data1.txt', delimiter=\",\")\n",
    "plotData(data[:, 0], data[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeCost(X, y, theta):\n",
    "    \"\"\"Function to compute the cost of linear regression\n",
    "    [Parameters]\n",
    "        X: Data matrix. X.shape = (m, n+1)\n",
    "        y: Target vector. y.shape = (m, 1)\n",
    "        theta: Hypothesis parameters. theta.shape = (n+1, 1)\n",
    "    [Returns]\n",
    "        J: (scalar) Cost\n",
    "    \"\"\"\n",
    "    J = 1/(2 * X.shape[0]) * np.sum(np.square(X.dot(theta) - y))\n",
    "    return J\n",
    "\n",
    "# convert y from (m,) to (m, 1)\n",
    "y = data[:, 1][:, np.newaxis]\n",
    "print(f'y.shape = {y.shape}')\n",
    "X = data[:, 0]\n",
    "m = X.shape[0]\n",
    "    \n",
    "# Add an additional first column to X and set it to all ones. \n",
    "# This allows us to treat θ0 as simply another ‘feature’.\n",
    "X = np.c_[np.ones(m), X]\n",
    "print(f'X.shape (appending X0) = {X.shape}')\n",
    "\n",
    "# initialize the initial hypothesis parameters to 0 and the learning rate alpha to 0.01\n",
    "theta = np.zeros((X.shape[1], 1))\n",
    "print(f'theta.shape = {theta.shape}')\n",
    "\n",
    "# Run computeCost once using θ initialized to zeros, and you will see a cost of 32.07\n",
    "print(f'Initial J(θ) = {computeCost(X, y, theta):.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientDescent(X, y, theta, alpha, iterations):\n",
    "    \"\"\"Function to run gradient descent\n",
    "    [Parameters]\n",
    "        X: Data matrix. X.shape = (m, n+1)\n",
    "        y: Target vector. y.shape = (m, 1)\n",
    "        theta: Hypothesis parameters. theta.shape = (n+1, 1)\n",
    "        alpha: Learning rate\n",
    "        iterations: Number of iterations to update theta\n",
    "    [Returns]\n",
    "        theta: Hypothesis parameters. theta.shape = (n+1, 1)\n",
    "        J: Cost history. J.shape = (iterations,)\n",
    "    \"\"\"\n",
    "    J = np.zeros(iterations)\n",
    "    for i in range(iterations):\n",
    "        J[i] = computeCost(X, y, theta)\n",
    "        theta = theta - alpha / X.shape[0] * X.T.dot(X.dot(theta) - y)\n",
    "    return theta, J\n",
    "\n",
    "theta = np.zeros((X.shape[1], 1))\n",
    "alpha = 0.01\n",
    "iterations = 1500\n",
    "theta, J = gradientDescent(X, y, theta, alpha, iterations)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(8, 3))\n",
    "axes[0].plot(np.arange(iterations), J, c='blue')\n",
    "axes[0].set_xlabel('Iterations')\n",
    "axes[0].set_ylabel('Cost')\n",
    "axes[0].set_title(f'alpha = {alpha}')\n",
    "\n",
    "## =================== Evaluation ===================\n",
    "axes[1].scatter(data[:, 0], data[:, 1], marker='x', c='r', s=20, label='training data')\n",
    "profits = [theta[0] + theta[1] * i for i in range(25)]\n",
    "axes[1].plot(np.arange(25), profits, c='blue', label='linear regression')\n",
    "axes[1].legend()\n",
    "axes[1].set_xlabel('Population of City in 10,000s')\n",
    "axes[1].set_ylabel('Profit in $10,000')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations on $\\theta$ vs. $J(\\theta)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta0 = np.linspace(-10, 10, 201)\n",
    "theta1 = np.linspace(-1, 4, 101)\n",
    "J_theta = np.zeros((len(theta0), len(theta1)))\n",
    "for i in range(len(theta0)):\n",
    "    for j in range(len(theta1)):\n",
    "        t = np.r_[theta0[i], theta1[j]][:, np.newaxis]\n",
    "        J_theta[i, j] = computeCost(X, y, t)\n",
    "\n",
    "# 3D surface plot\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "fig = plt.figure(figsize=(6, 4))\n",
    "ax = fig.gca(projection='3d')\n",
    "theta0, theta1 = np.meshgrid(theta0, theta1)\n",
    "print(f'theta0.shape = {theta0.shape}')\n",
    "print(f'theta1.shape = {theta1.shape}')\n",
    "print(f'J_theta.shape = {J_theta.shape}')\n",
    "surf = ax.plot_surface(theta0, theta1, J_theta.T, cmap=plt.cm.jet)\n",
    "cbar = plt.colorbar(surf, shrink=0.7)\n",
    "cbar.ax.set_ylabel('Cost')\n",
    "plt.xlabel('theta0')\n",
    "plt.ylabel('theta1')\n",
    "ax.view_init(azim=225)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contour plot\n",
    "theta_optimal = theta\n",
    "plt.plot(theta_optimal[0], theta_optimal[1], color='red', marker='x', markersize=5, linewidth=1)\n",
    "CS1 = plt.contourf(theta0, theta1, J_theta.T, levels=np.logspace(-2, 3, 10), cmap=plt.cm.coolwarm)\n",
    "CS2 = plt.contour(CS1, levels=CS1.levels[::1], linestyles=':', linewidths=1, colors='r')\n",
    "plt.title('This is title')\n",
    "plt.xlabel('theta0')\n",
    "plt.ylabel('theta1')\n",
    "\n",
    "# Make a colorbar for the ContourSet returned by the contourf call.\n",
    "cbar = plt.colorbar(CS1)\n",
    "cbar.ax.set_ylabel('cost')\n",
    "# Add the contour line levels to the colorbar\n",
    "cbar.add_lines(CS2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = np.zeros((X.shape[1], 1))\n",
    "alpha = 0.01\n",
    "iterations = 10000\n",
    "theta_optimal, J = gradientDescent(X, y, theta, alpha, iterations)\n",
    "print(f'Gradient Descent Search, theta =\\n{theta_optimal}')\n",
    "\n",
    "theta_normal = np.linalg.inv(X.T.dot(X)).dot(X.T.dot(y))\n",
    "print(f'Normal Equation, theta =\\n{theta_normal}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(data[:, 0][:, np.newaxis], data[:, 1])\n",
    "\n",
    "print(f'LinearRegression interception: {lin_reg.intercept_}')\n",
    "print(f'LinearRegression coefficients: {lin_reg.coef_}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
